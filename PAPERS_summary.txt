we have read the APPS and understood that: 
-the novelty of the database is that it aims at providing a benchmark tool to evaluate how well a language model can generate python, rather than simply translate. this is embedded by 
the fact tha teach example has multiples test cases and groun turth solutions. 
-the state of the art generative models (GPT-2 GPT-3 and GPT-Neo) show poor performances even on introductory problems.
-GPT-2 is not pretrained on code so they pretrained it on github . then they fine-tuned both gpt-2 and gpt-neosince the weights are available, but not GPT-3.
-finetuned improves model, which reduces amount of syntax errors, having GPT-Neo reaching over 5% of success with introductory problems. 

have read the human_eval paper: 
-SECTION 3: they have run 1-shot coedx on APPS, with filtered or unfiltered pass@k metrics, and compared it with results of fine-tuned GPT-Neo (raw pass) 
-SECTION 4: codex finetuning on a different dataset ( not many information on it and not opensource )

