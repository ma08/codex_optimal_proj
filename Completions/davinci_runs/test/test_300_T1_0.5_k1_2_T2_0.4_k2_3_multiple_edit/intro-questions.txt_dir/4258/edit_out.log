2022-04-22 20:54:03.977092 num operations 2
2022-04-22 20:54:03.977117 size on input set 1
2022-04-22 20:54:03.977312 using api key jaanu
2022-04-22 20:54:49.031883 fix spelling mistakes {
  "choices": [
    {
      "index": 0,
      "text": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data  ;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != '\\0') {\n        if (text[i] == '\\n') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == '.') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == ' ') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (word_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL) {\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_PARAGRAPHS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_PARAGRAPHS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        }\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = '\\0';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf(\"%s\", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(\" \");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(\".\");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1)\n            printf(\"\\n\");\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf(\"%d\", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf(\"%[^\\n]%*c\", p[i]);\n        strcat(doc, p[i]);\n        if (i != paragraph_count - 1)\n            strcat(doc, \"\\n\");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf(\"%d\", &q);\n\n    while (q--) {\n        int type;\n        scanf(\"%d\", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf(\"%d %d %d\", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf(\"%d %d\", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf(\"%d\", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf(\"\\n\");\n    }\n}\n"
    },
    {
      "index": 1,
      "text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f'/home/{os.environ.get(\"USER\")}/PythonLibrary')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = '../output/1101-1.csv.gz'\n\nCOMMENT = 'nejumi + f001'\n\nEXE_SUBMIT = True\n\n#DROP = ['f001_hostgal_specz']\n\nSEED = np.random.randint(9999)\nprint('SEED:', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         'objective': 'multiclass',\n         'num_class': 14,\n         'metric': 'multi_logloss',\n         \n         'learning_rate': 0.5,\n         'max_depth': 3,\n         'num_leaves': 63,\n         'max_bin': 255,\n         \n         'min_child_weight': 10,\n         'min_data_in_leaf': 150,\n         'reg_lambda': 0.5,  # L2 regularization term on weights.\n         'reg_alpha': 0.5,  # L1 regularization term on weights.\n         \n         'colsample_bytree': 0.5,\n         'subsample': 0.5,\n#         'nthread': 32,\n         'nthread': cpu_count(),\n         'bagging_freq': 1,\n         'verbose':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles(['../feature/train_'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles('../data/label').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f'duplicated!: { X_train.columns[X_train.columns.duplicated()] }')\nprint('no dup :) ')\nprint(f'X_train.shape {X_train.shape}')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f'CAT: {CAT}')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f\"CV auc-mean({NFOLD}): {ret['multi_logloss-mean'][-1]} + {ret['multi_logloss-stdv'][-1]}\"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param['seed'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret['multi_logloss-mean'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f\"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}\"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp['split'] /= imp['split'].max()\nimp['gain'] /= imp['gain'].max()\nimp['total'] = imp['split'] + imp['gain']\n\nimp.sort_values('total', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f'LOG/imp_{__file__}.csv', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles('../data/test_old')\n\ngc.collect()\n\nsub = pd.read_pickle('../data/sub.p')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub['target'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression='gzip')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print('submit')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n\n"
    },
    {
      "index": 2,
      "text": "#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, 'r')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, 'w')\n            outfile.write(re.sub(r\"\\b(a|A)n (\\w+)\", r\"\\1 \\2\", content))\n            outfile.close()\n\nif __name__ == '__main__':\n    main()\n"
    }
  ],
  "created": 1650660844,
  "object": "edit"
}
2022-04-22 20:54:49.032323 fix spelling mistakes 3 ['#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data  ;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != \'\\0\') {\n        if (text[i] == \'\\n\') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \'.\') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \' \') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (word_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL) {\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_PARAGRAPHS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_PARAGRAPHS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        }\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = \'\\0\';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf("%s", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(" ");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(".");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1)\n            printf("\\n");\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf("%d", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf("%[^\\n]%*c", p[i]);\n        strcat(doc, p[i]);\n        if (i != paragraph_count - 1)\n            strcat(doc, "\\n");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf("%d", &q);\n\n    while (q--) {\n        int type;\n        scanf("%d", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf("%d %d %d", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf("%d %d", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf("%d", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf("\\n");\n    }\n}\n', '#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n"""\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f\'/home/{os.environ.get("USER")}/PythonLibrary\')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = \'../output/1101-1.csv.gz\'\n\nCOMMENT = \'nejumi + f001\'\n\nEXE_SUBMIT = True\n\n#DROP = [\'f001_hostgal_specz\']\n\nSEED = np.random.randint(9999)\nprint(\'SEED:\', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         \'objective\': \'multiclass\',\n         \'num_class\': 14,\n         \'metric\': \'multi_logloss\',\n         \n         \'learning_rate\': 0.5,\n         \'max_depth\': 3,\n         \'num_leaves\': 63,\n         \'max_bin\': 255,\n         \n         \'min_child_weight\': 10,\n         \'min_data_in_leaf\': 150,\n         \'reg_lambda\': 0.5,  # L2 regularization term on weights.\n         \'reg_alpha\': 0.5,  # L1 regularization term on weights.\n         \n         \'colsample_bytree\': 0.5,\n         \'subsample\': 0.5,\n#         \'nthread\': 32,\n         \'nthread\': cpu_count(),\n         \'bagging_freq\': 1,\n         \'verbose\':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles([\'../feature/train_\'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles(\'../data/label\').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f\'duplicated!: { X_train.columns[X_train.columns.duplicated()] }\')\nprint(\'no dup :) \')\nprint(f\'X_train.shape {X_train.shape}\')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f\'CAT: {CAT}\')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f"CV auc-mean({NFOLD}): {ret[\'multi_logloss-mean\'][-1]} + {ret[\'multi_logloss-stdv\'][-1]}"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param[\'seed\'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret[\'multi_logloss-mean\'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp[\'split\'] /= imp[\'split\'].max()\nimp[\'gain\'] /= imp[\'gain\'].max()\nimp[\'total\'] = imp[\'split\'] + imp[\'gain\']\n\nimp.sort_values(\'total\', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f\'LOG/imp_{__file__}.csv\', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles(\'../data/test_old\')\n\ngc.collect()\n\nsub = pd.read_pickle(\'../data/sub.p\')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub[\'target\'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression=\'gzip\')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print(\'submit\')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n\n', '#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, \'r\')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, \'w\')\n            outfile.write(re.sub(r"\\b(a|A)n (\\w+)", r"\\1 \\2", content))\n            outfile.close()\n\nif __name__ == \'__main__\':\n    main()\n']
2022-04-22 20:54:49.032472 size on output set 3
2022-04-22 20:54:49.032481 size on input set 3
2022-04-22 20:54:49.032624 using api key jaanu
2022-04-22 20:54:54.864030 fix syntax errors {
  "choices": [
    {
      "index": 0,
      "text": "#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, 'r')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, 'w')\n            outfile.write(re.sub(r\"\\b(a|A)n (\\w+)\", r\"\\1 \\2\", content)\n            outfile.close()\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 1,
      "text": "#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, 'r')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, 'w')\n            outfile.write(re.sub(r\"\\b(a|A)n (\\w+)\", r\"\\1 \\2\", content)\n            outfile.close()\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 2,
      "text": "#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, 'r')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, 'w')\n            outfile.write(re.sub(r\"\\b(a|A)n (\\w+)\", r\"\\1 \\2\", content)) # re.sub(pattern, repl, string)\n            outfile.close()\n\nif __name__ == '__main__':\n    main()\n"
    }
  ],
  "created": 1650660889,
  "object": "edit"
}
2022-04-22 20:54:54.864268 fix syntax errors 3 ['#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, \'r\')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, \'w\')\n            outfile.write(re.sub(r"\\b(a|A)n (\\w+)", r"\\1 \\2", content)\n            outfile.close()\n\nif __name__ == \'__main__\':\n    main()\n', '#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, \'r\')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, \'w\')\n            outfile.write(re.sub(r"\\b(a|A)n (\\w+)", r"\\1 \\2", content)\n            outfile.close()\n\nif __name__ == \'__main__\':\n    main()\n', '#!/usr/bin/python\n\nimport sys\nimport os\nimport re\n\ndef main():\n    if len(sys.argv) > 1:\n        filename = sys.argv[1]\n        if os.path.isfile(filename):\n            infile = open(filename, \'r\')\n            content = infile.read()\n            infile.close()\n\n            outfile = open(filename, \'w\')\n            outfile.write(re.sub(r"\\b(a|A)n (\\w+)", r"\\1 \\2", content)) # re.sub(pattern, repl, string)\n            outfile.close()\n\nif __name__ == \'__main__\':\n    main()\n']
2022-04-22 20:54:54.864442 using api key sunu
2022-04-22 20:55:08.006177 fix syntax errors {
  "choices": [
    {
      "index": 0,
      "text": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data  ;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != '\\0') {\n        if (text[i] == '\\n') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == '.') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == ' ') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_SENTENCES) {\n            break;\n        }\n        if (word_index == MAX_WORDS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL) {\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_SENTENCES);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_WORDS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        }\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = '\\0';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf(\"%s\", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(\" \");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(\".\");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1)\n            printf(\"\\n\");\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf(\"%d\", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf(\"%[^\\n]%*c\", p[i]);\n        strcat(doc, p[i]);\n        if (i != paragraph_count - 1)\n            strcat(doc, \"\\n\");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf(\"%d\", &q);\n\n    while (q--) {\n        int type;\n        scanf(\"%d\", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf(\"%d %d %d\", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf(\"%d %d\", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf(\"%d\", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf(\"\\n\");\n    }\n}\n"
    },
    {
      "index": 1,
      "text": "#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != '\\0') {\n        if (text[i] == '\\n') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == '.') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == ' ') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (word_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL)\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_PARAGRAPHS);\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL)\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_PARAGRAPHS);\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL)\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = '\\0';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf(\"%s\", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(\" \");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(\".\");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1) {\n            printf(\"\\n\");\n        }\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf(\"%d\", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf(\"%[^\\n]%*c\", p[i]);\n        strcat(doc, p[i]);\n        }\n        if (i != paragraph_count - 1) {\n            strcat(doc, \"\\n\");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf(\"%d\", &q);\n\n    while (q--) {\n        int type;\n        scanf(\"%d\", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf(\"%d %d %d\", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf(\"%d %d\", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf(\"%d\", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf(\"\\n\");\n    }\n}\n"
    },
    {
      "error": {
        "message": "Could not edit text. Please sample again or try with a different temperature setting, input, or instruction.",
        "type": "invalid_edit"
      },
      "index": 2
    }
  ],
  "created": 1650660894,
  "object": "edit"
}
2022-04-22 20:55:08.006550 NO RESULT
2022-04-22 20:55:08.006561 {
  "error": {
    "message": "Could not edit text. Please sample again or try with a different temperature setting, input, or instruction.",
    "type": "invalid_edit"
  },
  "index": 2
}
2022-04-22 20:55:08.006610 fix syntax errors 2 ['#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data  ;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != \'\\0\') {\n        if (text[i] == \'\\n\') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \'.\') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \' \') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_SENTENCES) {\n            break;\n        }\n        if (word_index == MAX_WORDS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL) {\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_SENTENCES);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_WORDS);\n        }\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL) {\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        }\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = \'\\0\';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf("%s", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(" ");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(".");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1)\n            printf("\\n");\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf("%d", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf("%[^\\n]%*c", p[i]);\n        strcat(doc, p[i]);\n        if (i != paragraph_count - 1)\n            strcat(doc, "\\n");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf("%d", &q);\n\n    while (q--) {\n        int type;\n        scanf("%d", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf("%d %d %d", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf("%d %d", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf("%d", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf("\\n");\n    }\n}\n', '#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <assert.h>\n#include <ctype.h>\n\n#define MAX_CHARACTERS 1005\n#define MAX_PARAGRAPHS 5\n\nstruct word {\n    char* data;\n};\n\nstruct sentence {\n    struct word* data;\n    int word_count;//denotes number of words in a sentence\n};\n\nstruct paragraph {\n    struct sentence* data;\n    int sentence_count;//denotes number of sentences in a paragraph\n};\n\nstruct document {\n    struct paragraph* data;\n    int paragraph_count;//denotes number of paragraphs in a document\n};\n\nstruct document get_document(char* text) {\n    struct document doc;\n    doc.paragraph_count = 0;\n    doc.data = malloc(sizeof(struct paragraph)*MAX_PARAGRAPHS);\n    int paragraph_index = 0;\n    int sentence_index = 0;\n    int word_index = 0;\n    int i = 0;\n    while (text[i] != \'\\0\') {\n        if (text[i] == \'\\n\') {\n            doc.data[paragraph_index].sentence_count = sentence_index;\n            doc.paragraph_count++;\n            paragraph_index++;\n            sentence_index = 0;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \'.\') {\n            doc.data[paragraph_index].data[sentence_index].word_count = word_index;\n            sentence_index++;\n            word_index = 0;\n            i++;\n            continue;\n        }\n        if (text[i] == \' \') {\n            word_index++;\n            i++;\n            continue;\n        }\n        if (paragraph_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (sentence_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (word_index == MAX_PARAGRAPHS) {\n            break;\n        }\n        if (doc.data[paragraph_index].data == NULL)\n            doc.data[paragraph_index].data = malloc(sizeof(struct sentence)*MAX_PARAGRAPHS);\n        if (doc.data[paragraph_index].data[sentence_index].data == NULL)\n            doc.data[paragraph_index].data[sentence_index].data = malloc(sizeof(struct word)*MAX_PARAGRAPHS);\n        if (doc.data[paragraph_index].data[sentence_index].data[word_index].data == NULL)\n            doc.data[paragraph_index].data[sentence_index].data[word_index].data = malloc(sizeof(char)*MAX_CHARACTERS);\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[0] = text[i];\n        doc.data[paragraph_index].data[sentence_index].data[word_index].data[1] = \'\\0\';\n        i++;\n    }\n    return doc;\n}\n\nstruct word kth_word_in_mth_sentence_of_nth_paragraph(struct document Doc, int k, int m, int n) {\n    return Doc.data[n-1].data[m-1].data[k-1];\n}\n\nstruct sentence kth_sentence_in_mth_paragraph(struct document Doc, int k, int m) {\n    return Doc.data[m-1].data[k-1];\n}\n\nstruct paragraph kth_paragraph(struct document Doc, int k) {\n    return Doc.data[k-1];\n}\n\nvoid print_word(struct word w) {\n    printf("%s", w.data);\n}\n\nvoid print_sentence(struct sentence sen) {\n    for(int i = 0; i < sen.word_count; i++) {\n        print_word(sen.data[i]);\n        if (i != sen.word_count - 1) {\n            printf(" ");\n        }\n    }\n}\n\nvoid print_paragraph(struct paragraph para) {\n    for(int i = 0; i < para.sentence_count; i++){\n        print_sentence(para.data[i]);\n        printf(".");\n    }\n}\n\nvoid print_document(struct document doc) {\n    for(int i = 0; i < doc.paragraph_count; i++) {\n        print_paragraph(doc.data[i]);\n        if (i != doc.paragraph_count - 1) {\n            printf("\\n");\n        }\n    }\n}\n\nchar* get_input_text() {\n    int paragraph_count;\n    scanf("%d", &paragraph_count);\n\n    char p[MAX_PARAGRAPHS][MAX_CHARACTERS], doc[MAX_CHARACTERS];\n    memset(doc, 0, sizeof(doc));\n    getchar();\n    for (int i = 0; i < paragraph_count; i++) {\n        scanf("%[^\\n]%*c", p[i]);\n        strcat(doc, p[i]);\n        }\n        if (i != paragraph_count - 1) {\n            strcat(doc, "\\n");\n    }\n\n    char* returnDoc = (char*)malloc((strlen (doc)+1) * (sizeof(char)));\n    strcpy(returnDoc, doc);\n    return returnDoc;\n}\n\nint main()\n{\n    char* text = get_input_text();\n    struct document Doc = get_document(text);\n\n    int q;\n    scanf("%d", &q);\n\n    while (q--) {\n        int type;\n        scanf("%d", &type);\n\n        if (type == 3){\n            int k, m, n;\n            scanf("%d %d %d", &k, &m, &n);\n            struct word w = kth_word_in_mth_sentence_of_nth_paragraph(Doc, k, m, n);\n            print_word(w);\n        }\n\n        else if (type == 2) {\n            int k, m;\n            scanf("%d %d", &k, &m);\n            struct sentence sen= kth_sentence_in_mth_paragraph(Doc, k, m);\n            print_sentence(sen);\n        }\n\n        else{\n            int k;\n            scanf("%d", &k);\n            struct paragraph para = kth_paragraph(Doc, k);\n            print_paragraph(para);\n        }\n        printf("\\n");\n    }\n}\n']
2022-04-22 20:55:08.006892 using api key sunu
2022-04-22 20:55:14.614880 fix syntax errors {
  "choices": [
    {
      "index": 0,
      "text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f'/home/{os.environ.get(\"USER\")}/PythonLibrary')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = '../output/1101-1.csv.gz'\n\nCOMMENT = 'nejumi + f001'\n\nEXE_SUBMIT = True\n\n#DROP = ['f001_hostgal_specz']\n\nSEED = np.random.randint(9999)\nprint('SEED:', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         'objective': 'multiclass',\n         'num_class': 14,\n         'metric': 'multi_logloss',\n         \n         'learning_rate': 0.5,\n         'max_depth': 3,\n         'num_leaves': 63,\n         'max_bin': 255,\n         \n         'min_child_weight': 10,\n         'min_data_in_leaf': 150,\n         'reg_lambda': 0.5,  # L2 regularization term on weights.\n         'reg_alpha': 0.5,  # L1 regularization term on weights.\n         \n         'colsample_bytree': 0.5,\n         'subsample': 0.5,\n#         'nthread': 32,\n         'nthread': cpu_count(),\n         'bagging_freq': 1,\n         'verbose':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles(['../feature/train_'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles('../data/label').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f'duplicated!: { X_train.columns[X_train.columns.duplicated()] }')\nprint('no dup :) ')\nprint(f'X_train.shape {X_train.shape}')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f'CAT: {CAT}')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f\"CV auc-mean({NFOLD}): {ret['multi_logloss-mean'][-1]} + {ret['multi_logloss-stdv'][-1]}\"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param['seed'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret['multi_logloss-mean'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f\"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}\"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp['split'] /= imp['split'].max()\nimp['gain'] /= imp['gain'].max()\nimp['total'] = imp['split'] + imp['gain']\n\nimp.sort_values('total', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f'LOG/imp_{__file__}.csv', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles('../data/test_old')\n\ngc.collect()\n\nsub = pd.read_pickle('../data/sub.p')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub['target'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression='gzip')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print('submit')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n"
    },
    {
      "index": 1,
      "text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f'/home/{os.environ.get(\"USER\")}/PythonLibrary')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = '../output/1101-1.csv.gz'\n\nCOMMENT = 'nejumi + f001'\n\nEXE_SUBMIT = True\n\n#DROP = ['f001_hostgal_specz']\n\nSEED = np.random.randint(9999)\nprint('SEED:', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         'objective': 'multiclass',\n         'num_class': 14,\n         'metric': 'multi_logloss',\n         \n         'learning_rate': 0.5,\n         'max_depth': 3,\n         'num_leaves': 63,\n         'max_bin': 255,\n         \n         'min_child_weight': 10,\n         'min_data_in_leaf': 150,\n         'reg_lambda': 0.5,  # L2 regularization term on weights.\n         'reg_alpha': 0.5,  # L1 regularization term on weights.\n         \n         'colsample_bytree': 0.5,\n         'subsample': 0.5,\n#         'nthread': 32,\n         'nthread': cpu_count(),\n         'bagging_freq': 1,\n         'verbose':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles(['../feature/train_'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles('../data/label').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f'duplicated!: { X_train.columns[X_train.columns.duplicated()] }')\nprint('no dup :) ')\nprint(f'X_train.shape {X_train.shape}')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f'CAT: {CAT}')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f\"CV auc-mean({NFOLD}): {ret['multi_logloss-mean'][-1]} + {ret['multi_logloss-stdv'][-1]}\"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param['seed'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) ) # TODO: wloss\n    model_all += models\n    nround_mean += len(ret['multi_logloss-mean'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f\"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}\"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp['split'] /= imp['split'].max()\nimp['gain'] /= imp['gain'].max()\nimp['total'] = imp['split'] + imp['gain']\n\nimp.sort_values('total', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f'LOG/imp_{__file__}.csv', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles('../data/test_old')\n\ngc.collect()\n\nsub = pd.read_pickle('../data/sub.p')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub['target'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression='gzip')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print('submit')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n"
    },
    {
      "index": 2,
      "text": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f'/home/{os.environ.get(\"USER\")}/PythonLibrary')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = '../output/1101-1.csv.gz'\n\nCOMMENT = 'nejumi + f001'\n\nEXE_SUBMIT = True\n\n#DROP = ['f001_hostgal_specz']\n\nSEED = np.random.randint(9999)\nprint('SEED:', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         'objective': 'multiclass',\n         'num_class': 14,\n         'metric': 'multi_logloss',\n         \n         'learning_rate': 0.5,\n         'max_depth': 3,\n         'num_leaves': 63,\n         'max_bin': 255,\n         \n         'min_child_weight': 10,\n         'min_data_in_leaf': 150,\n         'reg_lambda': 0.5,  # L2 regularization term on weights.\n         'reg_alpha': 0.5,  # L1 regularization term on weights.\n         \n         'colsample_bytree': 0.5,\n         'subsample': 0.5,\n#         'nthread': 32,\n         'nthread': cpu_count(),\n         'bagging_freq': 1,\n         'verbose':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles(['../feature/train_'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles('../data/label').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f'duplicated!: { X_train.columns[X_train.columns.duplicated()] }')\nprint('no dup :) ')\nprint(f'X_train.shape {X_train.shape}')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f'CAT: {CAT}')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f\"CV auc-mean({NFOLD}): {ret['multi_logloss-mean'][-1]} + {ret['multi_logloss-stdv'][-1]}\"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param['seed'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=14)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret['multi_logloss-mean'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f\"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}\"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp['split'] /= imp['split'].max()\nimp['gain'] /= imp['gain'].max()\nimp['total'] = imp['split'] + imp['gain']\n\nimp.sort_values('total', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f'LOG/imp_{__file__}.csv', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles('../data/test_old')\n\ngc.collect()\n\nsub = pd.read_pickle('../data/sub.p')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub['target'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression='gzip')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print('submit')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n"
    }
  ],
  "created": 1650660908,
  "object": "edit"
}
2022-04-22 20:55:14.615299 fix syntax errors 3 ['#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n"""\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f\'/home/{os.environ.get("USER")}/PythonLibrary\')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = \'../output/1101-1.csv.gz\'\n\nCOMMENT = \'nejumi + f001\'\n\nEXE_SUBMIT = True\n\n#DROP = [\'f001_hostgal_specz\']\n\nSEED = np.random.randint(9999)\nprint(\'SEED:\', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         \'objective\': \'multiclass\',\n         \'num_class\': 14,\n         \'metric\': \'multi_logloss\',\n         \n         \'learning_rate\': 0.5,\n         \'max_depth\': 3,\n         \'num_leaves\': 63,\n         \'max_bin\': 255,\n         \n         \'min_child_weight\': 10,\n         \'min_data_in_leaf\': 150,\n         \'reg_lambda\': 0.5,  # L2 regularization term on weights.\n         \'reg_alpha\': 0.5,  # L1 regularization term on weights.\n         \n         \'colsample_bytree\': 0.5,\n         \'subsample\': 0.5,\n#         \'nthread\': 32,\n         \'nthread\': cpu_count(),\n         \'bagging_freq\': 1,\n         \'verbose\':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles([\'../feature/train_\'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles(\'../data/label\').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f\'duplicated!: { X_train.columns[X_train.columns.duplicated()] }\')\nprint(\'no dup :) \')\nprint(f\'X_train.shape {X_train.shape}\')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f\'CAT: {CAT}\')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f"CV auc-mean({NFOLD}): {ret[\'multi_logloss-mean\'][-1]} + {ret[\'multi_logloss-stdv\'][-1]}"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param[\'seed\'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret[\'multi_logloss-mean\'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp[\'split\'] /= imp[\'split\'].max()\nimp[\'gain\'] /= imp[\'gain\'].max()\nimp[\'total\'] = imp[\'split\'] + imp[\'gain\']\n\nimp.sort_values(\'total\', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f\'LOG/imp_{__file__}.csv\', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles(\'../data/test_old\')\n\ngc.collect()\n\nsub = pd.read_pickle(\'../data/sub.p\')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub[\'target\'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression=\'gzip\')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print(\'submit\')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n', '#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n"""\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f\'/home/{os.environ.get("USER")}/PythonLibrary\')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = \'../output/1101-1.csv.gz\'\n\nCOMMENT = \'nejumi + f001\'\n\nEXE_SUBMIT = True\n\n#DROP = [\'f001_hostgal_specz\']\n\nSEED = np.random.randint(9999)\nprint(\'SEED:\', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         \'objective\': \'multiclass\',\n         \'num_class\': 14,\n         \'metric\': \'multi_logloss\',\n         \n         \'learning_rate\': 0.5,\n         \'max_depth\': 3,\n         \'num_leaves\': 63,\n         \'max_bin\': 255,\n         \n         \'min_child_weight\': 10,\n         \'min_data_in_leaf\': 150,\n         \'reg_lambda\': 0.5,  # L2 regularization term on weights.\n         \'reg_alpha\': 0.5,  # L1 regularization term on weights.\n         \n         \'colsample_bytree\': 0.5,\n         \'subsample\': 0.5,\n#         \'nthread\': 32,\n         \'nthread\': cpu_count(),\n         \'bagging_freq\': 1,\n         \'verbose\':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles([\'../feature/train_\'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles(\'../data/label\').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f\'duplicated!: { X_train.columns[X_train.columns.duplicated()] }\')\nprint(\'no dup :) \')\nprint(f\'X_train.shape {X_train.shape}\')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f\'CAT: {CAT}\')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f"CV auc-mean({NFOLD}): {ret[\'multi_logloss-mean\'][-1]} + {ret[\'multi_logloss-stdv\'][-1]}"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param[\'seed\'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=True)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) ) # TODO: wloss\n    model_all += models\n    nround_mean += len(ret[\'multi_logloss-mean\'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp[\'split\'] /= imp[\'split\'].max()\nimp[\'gain\'] /= imp[\'gain\'].max()\nimp[\'total\'] = imp[\'split\'] + imp[\'gain\']\n\nimp.sort_values(\'total\', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f\'LOG/imp_{__file__}.csv\', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles(\'../data/test_old\')\n\ngc.collect()\n\nsub = pd.read_pickle(\'../data/sub.p\')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub[\'target\'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression=\'gzip\')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print(\'submit\')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n', '#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n"""\nCreated on Mon Nov  5 14:07:53 2018\n\n@author: kazuki.onodera\n"""\n\nimport numpy as np\nimport pandas as pd\nimport os, gc\nfrom glob import glob\nfrom tqdm import tqdm\n\nimport sys\nsys.path.append(f\'/home/{os.environ.get("USER")}/PythonLibrary\')\nimport lgbextension as ex\nimport lightgbm as lgb\nfrom multiprocessing import cpu_count\n\nimport utils , utils_cat\nutils.start(__file__)\n#==============================================================================\n\nSUBMIT_FILE_PATH = \'../output/1101-1.csv.gz\'\n\nCOMMENT = \'nejumi + f001\'\n\nEXE_SUBMIT = True\n\n#DROP = [\'f001_hostgal_specz\']\n\nSEED = np.random.randint(9999)\nprint(\'SEED:\', SEED)\n\nNFOLD = 5\n\nLOOP = 5\n\nparam = {\n         \'objective\': \'multiclass\',\n         \'num_class\': 14,\n         \'metric\': \'multi_logloss\',\n         \n         \'learning_rate\': 0.5,\n         \'max_depth\': 3,\n         \'num_leaves\': 63,\n         \'max_bin\': 255,\n         \n         \'min_child_weight\': 10,\n         \'min_data_in_leaf\': 150,\n         \'reg_lambda\': 0.5,  # L2 regularization term on weights.\n         \'reg_alpha\': 0.5,  # L1 regularization term on weights.\n         \n         \'colsample_bytree\': 0.5,\n         \'subsample\': 0.5,\n#         \'nthread\': 32,\n         \'nthread\': cpu_count(),\n         \'bagging_freq\': 1,\n         \'verbose\':-1,\n         }\n\n\nnp.random.seed(SEED)\n\nloader = utils_cat.LoadFiles([\'../feature/train_\'])\n\n\n# =============================================================================\n# load\n# =============================================================================\nX_train = loader.train()\ny_train = utils.read_pickles(\'../data/label\').values\n\nif X_train.columns.duplicated().sum()>0:\n    raise Exception(f\'duplicated!: { X_train.columns[X_train.columns.duplicated()] }\')\nprint(\'no dup :) \')\nprint(f\'X_train.shape {X_train.shape}\')\n\ngc.collect()\n\nCAT = list( set(X_train.columns) & set(loader.category()) )\nprint(f\'CAT: {CAT}\')\n\nCOL = X_train.columns.tolist()\n\n# =============================================================================\n# cv\n# =============================================================================\ndtrain = lgb.Dataset(X_train, y_train, categorical_feature=CAT )\ngc.collect()\n\nret = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n             early_stopping_rounds=100, verbose_eval=50,\n             seed=SEED)\n\nresult = f"CV auc-mean({NFOLD}): {ret[\'multi_logloss-mean\'][-1]} + {ret[\'multi_logloss-stdv\'][-1]}"\nprint(result)\n\nutils.send_line(result)\n\n\n# =============================================================================\n# train\n# =============================================================================\n\nmodel_all = []\nnround_mean = 0\nwloss_list = []\nfor i in range(LOOP):\n    gc.collect()\n    param[\'seed\'] = np.random.randint(9999)\n    ret, models = lgb.cv(param, dtrain, 9999, nfold=NFOLD,\n                         early_stopping_rounds=100, verbose_eval=50,\n                         seed=SEED)\n    y_pred = ex.eval_oob(X_train, y_train, models, SEED, stratified=True, shuffle=True, \n                         n_class=14)\n    wloss_list.append( log_loss(y_train, y_pred, labels = list(range(14))) )\n    model_all += models\n    nround_mean += len(ret[\'multi_logloss-mean\'])\n    gc.collect()\n\nnround_mean = int((nround_mean/LOOP) * 1.3)\n\nresult = f"CV wloss: {np.mean(wloss_list)} + {np.std(wloss_list)}"\nprint(result)\n\nutils.send_line(result)\n\nimp = ex.getImp(model_all)\nimp[\'split\'] /= imp[\'split\'].max()\nimp[\'gain\'] /= imp[\'gain\'].max()\nimp[\'total\'] = imp[\'split\'] + imp[\'gain\']\n\nimp.sort_values(\'total\', ascending=False, inplace=True)\nimp.reset_index(drop=True, inplace=True)\n\n\nimp.to_csv(f\'LOG/imp_{__file__}.csv\', index=False)\n\n\n# =============================================================================\n# test\n# =============================================================================\n\ndtest = utils.read_pickles(\'../data/test_old\')\n\ngc.collect()\n\nsub = pd.read_pickle(\'../data/sub.p\')\n\ny_pred = pd.Series(0, index=sub.index)\n\nfor model in tqdm(model_all):\n    y_pred += pd.Series(model.predict(dtest[COL])).rank()\ny_pred /= y_pred.max()\ny_pred /= LOOP\n\nsub[\'target\'] = y_pred.values\n\nsub.to_csv(SUBMIT_FILE_PATH, index=False, compression=\'gzip\')\n\n# =============================================================================\n# submission\n# =============================================================================\nif EXE_SUBMIT:\n    print(\'submit\')\n    utils.submit(SUBMIT_FILE_PATH, COMMENT)\n\n\n#==============================================================================\nutils.end(__file__)\n#utils.stop_instance()\n']
2022-04-22 20:55:14.615492 size on output set 7
2022-04-22 20:55:14.615509 num operations 2
2022-04-22 20:55:14.615515 size on input set 1
2022-04-22 20:55:14.615657 using api key sunu
2022-04-22 20:55:19.054356 fix spelling mistakes {
  "choices": [
    {
      "index": 0,
      "text": "\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 1,
      "text": "\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 2,
      "text": "\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n"
    }
  ],
  "created": 1650660914,
  "object": "edit"
}
2022-04-22 20:55:19.054570 fix spelling mistakes 3 ["\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n", "\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n", "\ndef main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n"]
2022-04-22 20:55:19.054595 size on output set 1
2022-04-22 20:55:19.054604 size on input set 1
2022-04-22 20:55:19.054744 using api key derek
2022-04-22 20:55:24.728423 fix syntax errors {
  "choices": [
    {
      "index": 0,
      "text": "\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 1,
      "text": "def main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
      "index": 2,
      "text": "def main():\n    a, b, t = map(int, input().split())\n    print(b * (t // a + 1))\n\n\nif __name__ == '__main__':\n    main()\n"
    }
  ],
  "created": 1650660919,
  "object": "edit"
}
2022-04-22 20:55:24.728644 fix syntax errors 3 ["\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n", "def main():\n    A, B, T = map(int, input().split())\n    print(B * (T // A + 1))\n\n\nif __name__ == '__main__':\n    main()\n", "def main():\n    a, b, t = map(int, input().split())\n    print(b * (t // a + 1))\n\n\nif __name__ == '__main__':\n    main()\n"]
2022-04-22 20:55:24.728667 size on output set 3
