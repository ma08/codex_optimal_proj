["\ns = input()\nt = input()\n\nans = 0\n\nfor i in range(len(s)):\n    if s[i] == t[i]:\n        ans += 1\n\nprint(correct)\n", "\ns = input()\nt = input()\n\ncorrect = 0\n\nfor i in range(len(s)):\n    if s[i] == t[i]:\n        correct += 1\n\n\n\n\n\n\nprint(correct)\n", "\nimport collections\nimport csv\nimport itertools\nimport os\nimport sys\nimport multiprocessing\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom util.default_util import *\nfrom util.embedding_util import *\nfrom util.model_util import *\n\n\ndef main(argv):\n    # load the arguments\n    args = default_parser().parse_args(argv[1:])\n\n    if args.mode == 'train':\n        train(args)\n    elif args.mode == 'test':\n        test(args)\n    elif args.mode == 'predict':\n        predict(args)\n    elif args.mode == 'evaluate':\n        evaluate(args)\n    else:\n        raise ValueError('Invalid mode: %s' % args.mode)\n\n\ndef train(args):\n    data = Dataset(args)\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # build the graph\n            initializer = tf.random_uniform_initializer(-args.init_scale, args.init_scale)\n            with tf.variable_scope('model', reuse=None, initializer=initializer):\n                model = Model(args, is_training=True)\n\n            # initialize\n            tf.global_variables_initializer().run()\n\n            # train\n            for i in range(args.num_epoch):\n                print('Training in epoch %d' % (i + 1))\n                data.shuffle()\n                epoch_size = data.get_num_batches(args.batch_size)\n                for step in range(epoch_size):\n                    x, y = data.next_batch(args.batch_size)\n                    feed_dict = {model.input_data: x, model.targets: y}\n                    _, loss, summary = sess.run([model.train_op, model.cost, model.merged], feed_dict=feed_dict)\n\n                    if step > 0 and step % args.print_every == 0:\n                        print('Epoch %d, step %d, loss = %.6f' % (i + 1, step, loss))\n\n                if args.tensorboard:\n                    model.writer.add_summary(summary, i)\n\n                # save the model\n                if (i + 1) % args.save_every == 0 or (i + 1) == args.num_epoch:\n                    checkpoint_path = os.path.join(args.model_dir, 'model.ckpt')\n                    model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n\n\ndef test(args):\n    data = Dataset(args)\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # build the graph\n            initializer = tf.random_uniform_initializer(-args.init_scale, args.init_scale)\n            with tf.variable_scope('model', reuse=None, initializer=initializer):\n                model = Model(args, is_training=False)\n\n            # initialize\n            tf.global_variables_initializer().run()\n\n            # load the model\n            model_path = os.path.join(args.model_dir, 'model.ckpt')\n            ckpt = tf.train.get_checkpoint_state(args.model_dir)\n            if ckpt and ckpt.model_checkpoint_path:\n                print('Loading the model from %s' % ckpt.model_checkpoint_path)\n                model.saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise ValueError('No checkpoint file found in %s' % args.model_dir)\n\n            # test\n            print('Testing...')\n            num_batches = data.get_num_batches(args.batch_size)\n            for step in range(num_batches):\n                x, y = data.next_batch(args.batch_size)\n                feed_dict = {model.input_data: x, model.targets: y}\n                loss = sess.run([model.cost], feed_dict=feed_dict)\n                print('Batch %d, loss = %.6f' % (step, loss))\n\n\ndef predict(args):\n    data = Dataset(args)\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # build the graph\n            initializer = tf.random_uniform_initializer(-args.init_scale, args.init_scale)\n            with tf.variable_scope('model', reuse=None, initializer=initializer):\n                model = Model(args, is_training=False)\n\n            # initialize\n            tf.global_variables_initializer().run()\n\n            # load the model\n            model_path = os.path.join(args.model_dir, 'model.ckpt')\n            ckpt = tf.train.get_checkpoint_state(args.model_dir)\n            if ckpt and ckpt.model_checkpoint_path:\n                print('Loading the model from %s' % ckpt.model_checkpoint_path)\n                model.saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise ValueError('No checkpoint file found in %s' % args.model_dir)\n\n            # predict\n            print('Predicting...')\n            num_batches = data.get_num_batches(args.batch_size)\n            for step in range(num_batches):\n                x, y = data.next_batch(args.batch_size)\n                feed_dict = {model.input_data: x, model.targets: y}\n                predictions = sess.run([model.predictions], feed_dict=feed_dict)\n                print(predictions)\n\n\ndef evaluate(args):\n    data = Dataset(args)\n\n    with tf.Graph().as_default():\n        with tf.Session() as sess:\n            # build the graph\n            initializer = tf.random_uniform_initializer(-args.init_scale, args.init_scale)\n            with tf.variable_scope('model', reuse=None, initializer=initializer):\n                model = Model(args, is_training=False)\n\n            # initialize\n            tf.global_variables_initializer().run()\n\n            # load the model\n            model_path = os.path.join(args.model_dir, 'model.ckpt')\n            ckpt = tf.train.get_checkpoint_state(args.model_dir)\n            if ckpt and ckpt.model_checkpoint_path:\n                print('Loading the model from %s' % ckpt.model_checkpoint_path)\n                model.saver.restore(sess, ckpt.model_checkpoint_path)\n            else:\n                raise ValueError('No checkpoint file found in %s' % args.model_dir)\n\n            # evaluate\n            print('Evaluating...')\n            num_batches = data.get_num_batches(args.batch_size)\n            for step in range(num_batches):\n                x, y = data.next_batch(args.batch_size)\n                feed_dict = {model.input_data:\n", "\ns = input()\nt = input()\n\ncorrect = 0\n\nfor i in range(len(s)):\n    if s[i] == t[i]:\n        correct += 1\n\nprint(correct)\n", "\n\ns = input()\nt = input()\n\ncorrect = 0\n\nfor i in range(len(s)):\t\t# if s[i] == t[i] then increment correct\n    if s[i] == t[i]:\n        correct += 1\n\nprint(correct)\n", "\n\ns = input()\nt = input()\ncorrect = 0\nfor i in range(len(s)):\n    if s[i] == t[i]:\n        correct += 1\nprint(correct)\n", "\n\ns = input().rstrip()\nt = input().rstrip()\n\ncorrect = 0\n\nfor i in range(len(s)):\n    if s[i] == t[i]:\n        correct += 1\n\nprint(correct)\n", "a = int(input())\nb = int(input())\nc = int(input())\nx = int(input())\n\n\nans = 0\n\nfor i in range(a+1):\n    for j in range(b+1):\n        for k in range(c+1):\n            total = (i * 500) + (j * 100) + (k * 50)\n            if total == x:\n                ans += 1\n\nprint(ans)\n", "\nimport sys\n\ndef main():\n    forecast = sys.stdin.readline().strip()\n    actual = sys.stdin.readline().strip()\n\n    correct = 0\n    for i in range(len(forecast)):\n        if forecast[i] == actual[i]:\n            correct += 1\n    return correct\n\nif __name__ == '__main__':\n    main()\n", "\nimport sys\n\ndef main():\n    forecast = sys.stdin.readline().strip()\n    actual = sys.stdin.readline().strip()\n\n    correct = 0\n    for i in range(len(forecast)):\n        if forecast[i] == actual[i]:\n            correct += 1\n    print(correct)\n\nif __name__ == '__main__':\n    main()\n"]