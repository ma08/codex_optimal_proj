import io
import json
import logging
import math
import numpy as np
import os
import pprint
import sys
import testing_util as test_util
import time

from tqdm import tqdm

from typing import List

def get_results(results, args):
    at_least_1sol= []
    tot_res = []
    for index in results:
       tot_res.extend(results[index])

       #This is the key function needed to calculate the pass@k metric.
       #It checks, for each prompt, that there is at least one array of all
       #True values in the list of arrays. This means that there is a solution
       #that passes all the test cases.

       at_least_1sol.append(np.all(np.all(results[index])))

    total_testcases = len(tot_res)
    print(f"number of test cases run = {total_testcases}")


    print("\n  At least one solution: ",at_least_1sol)
    print(f"\n Pass@k = {np.mean(at_least_1sol)}")


def evaluate_problems(args):

   #open json file containing all the target folders containing the prompts and the corresponding set 
   #of solutions generated by Codex Completions and stored in the subfolder codex_solutions
    with open(args.test_loc, "r") as f:
        problems = sorted(json.load(f))

    results = {}

    #save the path of the file where we want to store the evaluations results
    results_save = os.path.join(args.save, f"results.json") 

    # evaluation loop across the targeted prompts and corresponding solutions
    #OUTER LOOP ACOROSS PROMPTS
    for index, problem in enumerate(tqdm(problems)):
        print("Prompt #",index)

        #Load the solutions obtained with Codex Completions

        samples_path = os.path.join(problem, "codex_edit_solutions.json")
        # print("samples_path",samples_path)
        try:
          with open(samples_path, "r") as f:
            samples = json.load(f)

          res = []
        #INNER LOOP FOR SINGLE PROMPT
          for sample_idx, sample in enumerate(samples):  
            curr_res = [-2]
            try:

              #Run the evaluation: test the solution sample with all the test_cases provided by APPS
              #for the prompt from which the solution was generated. The value returned and stored in
              #curr_res will be an array of lenght equal to the number of test cases provided for the
              #prompt currently evaluated, where each value of the array can be:

              #-[True] = the solution passes the test case
              #-[False] = the solutions does not pass the test case

              #Note that in case the solution is not even compiling, or there is a runtime error, this
              #will hold for any test case, so insted of having an array we simply get a single value
              #respectively equal to [-2] for compile error and [-1] for runtime error.

                curr_res = test_util.run_test(prob_path=problem, test=sample)
                fixed = []
                for e in curr_res:
                    if isinstance(e, np.ndarray):
                       e = e.item(0)
                    if isinstance(e, np.bool_):
                        e = bool(e)
                    fixed.append(e)
                curr_res = fixed
                if not np.all(curr_res):
                    print(f"Results were not all True: {curr_res}")
            except Exception as e:
                print(f"test framework exception = {repr(e)}{e}\n")
                break
            finally:
                assert isinstance(curr_res, list)

                #append the array curr_res (results of this solution for each test case) in the res array.
                #In this way, once completed the inner loop, res is a list of array, one for each 
                #solution of the current prompt under test.
                res.append(curr_res)

        #store the res array, containing all the evaluation information for a single prompt, in the 
        #results array. In this way, once completed the outer loop across prompts, results contains all
        #evaluation informations for the batch of prompts targeted, and can be store in a dedicated
        #foler

          results[index] = res

          with open(results_save, "w") as f:
            try:
              f.write(json.dumps(results))
            except Exception as e:
              import pdb; pdb.set_trace()
              print("didn't save problem due to {e}")

        except:
          print("########could not open file#########")
          continue 


    return results


def main(args):

    argsdict = vars(args)
    print(pprint.pformat(argsdict))

    if args.print_results:
        results = {}
        results_save = os.path.join(args.save, f"results.json")
        with open(results_save, "r") as f:
            results = json.load(f)
    else:
        results = evaluate_problems(args)

    get_results(results, args)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Testing a Language Model on Python Code")
    parser.add_argument("-t","--test_loc", default="../data_split/test.json", type=str, help="path to the json containing problem paths to be evaluated.")
    parser.add_argument("-p", "--print_results", action="store_true", help="To use if you have already evaluated ")
    parser.add_argument("--save", type=str, default="./results", help="To use in order to set a specific target foler")

    args = parser.parse_args()

    main(args)
